{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8524786,"sourceType":"datasetVersion","datasetId":5090477}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning LLMs on fake-news dataset\n\n*(FYI: this notebook is designed to be run in Kaggle)*","metadata":{}},{"cell_type":"code","source":"! pip install -U trl peft accelerate bitsandbytes einops --quiet","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:45:18.978428Z","iopub.execute_input":"2024-06-01T16:45:18.978855Z","iopub.status.idle":"2024-06-01T16:45:38.967522Z","shell.execute_reply.started":"2024-06-01T16:45:18.978813Z","shell.execute_reply":"2024-06-01T16:45:38.966464Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    BitsAndBytesConfig,\n    T5Tokenizer, \n    T5ForConditionalGeneration, \n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    GenerationConfig\n)\n\nfrom datasets import Dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:11:42.134398Z","iopub.execute_input":"2024-06-01T17:11:42.134824Z","iopub.status.idle":"2024-06-01T17:11:42.141323Z","shell.execute_reply.started":"2024-06-01T17:11:42.134796Z","shell.execute_reply":"2024-06-01T17:11:42.140346Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Creation","metadata":{}},{"cell_type":"code","source":"CLEAN_TRAIN_DATASET_PATH = Path(\"../input/welfake-clean/WELFake_clean_train.csv\")\nRANDOM_SEED = 42\n\ndataset_df = pd.read_csv(CLEAN_TRAIN_DATASET_PATH, index_col=0)\ndataset_df","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:45:57.728254Z","iopub.execute_input":"2024-06-01T16:45:57.728807Z","iopub.status.idle":"2024-06-01T16:46:02.399810Z","shell.execute_reply.started":"2024-06-01T16:45:57.728780Z","shell.execute_reply":"2024-06-01T16:46:02.398677Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                   title  \\\n45905  Exclusive: Foreign Isis Fighters Defend Mosul ...   \n37291  JUDGE JEANINE UNLOADS On Hillary: “How Did You...   \n46730  Gunman attacks Saudi security forces at gate o...   \n66327  Indian Software Mogul: Hire Americans Now Beca...   \n58329  Rep. Diaz-Balart: Liberals Against Trump Who F...   \n...                                                  ...   \n37847  To applause and boos, Kerry urges Congress to ...   \n6384   TINGLE UP HIS LEG? NBC Paid Off Chris Matthews...   \n55885  U.S. government shares technical details on No...   \n881    Trumps history of corruption is mind-boggling....   \n16065  ’Facezam’ App Lets Strangers Stalk Your Facebo...   \n\n                                                    text  label  \n45905    \\nForeign fighters for Isis are choosing to ...      1  \n37291  You don t want to miss a second of Judge Jeani...      1  \n46730  RIYADH (Reuters) - Two Saudi guards were shot ...      0  \n66327  A leading Indian software entrepreneur says In...      0  \n58329  Florida Congressman Mario   attacked the “doub...      0  \n...                                                  ...    ...  \n37847  CHICAGO (Reuters) - Failure to approve the Tra...      0  \n6384   Here s yet another claim that s really iffy be...      1  \n55885  WASHINGTON (Reuters) - The U.S. government on ...      0  \n881    In the heat of a presidential campaign, youd t...      0  \n16065  Creepers of the web, rejoice! Now you can find...      0  \n\n[56634 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45905</th>\n      <td>Exclusive: Foreign Isis Fighters Defend Mosul ...</td>\n      <td>\\nForeign fighters for Isis are choosing to ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37291</th>\n      <td>JUDGE JEANINE UNLOADS On Hillary: “How Did You...</td>\n      <td>You don t want to miss a second of Judge Jeani...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>46730</th>\n      <td>Gunman attacks Saudi security forces at gate o...</td>\n      <td>RIYADH (Reuters) - Two Saudi guards were shot ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>66327</th>\n      <td>Indian Software Mogul: Hire Americans Now Beca...</td>\n      <td>A leading Indian software entrepreneur says In...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>58329</th>\n      <td>Rep. Diaz-Balart: Liberals Against Trump Who F...</td>\n      <td>Florida Congressman Mario   attacked the “doub...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>37847</th>\n      <td>To applause and boos, Kerry urges Congress to ...</td>\n      <td>CHICAGO (Reuters) - Failure to approve the Tra...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6384</th>\n      <td>TINGLE UP HIS LEG? NBC Paid Off Chris Matthews...</td>\n      <td>Here s yet another claim that s really iffy be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>55885</th>\n      <td>U.S. government shares technical details on No...</td>\n      <td>WASHINGTON (Reuters) - The U.S. government on ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>881</th>\n      <td>Trumps history of corruption is mind-boggling....</td>\n      <td>In the heat of a presidential campaign, youd t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16065</th>\n      <td>’Facezam’ App Lets Strangers Stalk Your Facebo...</td>\n      <td>Creepers of the web, rejoice! Now you can find...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>56634 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"short_dataset_df = dataset_df[dataset_df[\"text\"].str.split().str.len() <= 100]\nshort_dataset_df","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:46:02.404765Z","iopub.execute_input":"2024-06-01T16:46:02.405486Z","iopub.status.idle":"2024-06-01T16:46:06.151863Z","shell.execute_reply.started":"2024-06-01T16:46:02.405454Z","shell.execute_reply":"2024-06-01T16:46:06.150617Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   title  \\\n1290               'There appear to be no rules anymore'   \n68068  Spain to control Catalan spending as long as '...   \n48833   House committee postpones hearing on Puerto Rico   \n52423  Former New York City Mayor Bloomberg to endors...   \n37654  Newly Approved GM Potatoes Have Potential to S...   \n...                                                  ...   \n9876   HOW BAD IS IT IN VENEZUELA? Socialism’s Endgam...   \n36400  Wow! Must Watch Video Of Grilling Of Congressm...   \n26093  Trump's New Ad Portraying 'Every Mother's Wors...   \n5407   BREAKING: FEDERAL COURT RULES ON NSA’S WARRANT...   \n66148  China calls for restraint when asked about Nor...   \n\n                                                    text  label  \n1290   There is an path for Democrats to regain the p...      0  \n68068  MADRID (Reuters) - The Spanish government said...      0  \n48833  NEW YORK (Reuters) - The U.S. House of Represe...      0  \n52423  WASHINGTON (Reuters) - Former New York City Ma...      0  \n37654  By Whitney Webb Late last week, the US Departm...      1  \n...                                                  ...    ...  \n9876   How bad is it in Venezuela? People are eating ...      1  \n36400  Fournier totally hammers these two-great to see!       1  \n26093  Share on Twitter The Wildfire is an opinion pl...      1  \n5407   Another positive step towards restoring our fr...      1  \n66148  BEIJING (Reuters) - China called on all partie...      0  \n\n[6040 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1290</th>\n      <td>'There appear to be no rules anymore'</td>\n      <td>There is an path for Democrats to regain the p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>68068</th>\n      <td>Spain to control Catalan spending as long as '...</td>\n      <td>MADRID (Reuters) - The Spanish government said...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48833</th>\n      <td>House committee postpones hearing on Puerto Rico</td>\n      <td>NEW YORK (Reuters) - The U.S. House of Represe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52423</th>\n      <td>Former New York City Mayor Bloomberg to endors...</td>\n      <td>WASHINGTON (Reuters) - Former New York City Ma...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>37654</th>\n      <td>Newly Approved GM Potatoes Have Potential to S...</td>\n      <td>By Whitney Webb Late last week, the US Departm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9876</th>\n      <td>HOW BAD IS IT IN VENEZUELA? Socialism’s Endgam...</td>\n      <td>How bad is it in Venezuela? People are eating ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36400</th>\n      <td>Wow! Must Watch Video Of Grilling Of Congressm...</td>\n      <td>Fournier totally hammers these two-great to see!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26093</th>\n      <td>Trump's New Ad Portraying 'Every Mother's Wors...</td>\n      <td>Share on Twitter The Wildfire is an opinion pl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5407</th>\n      <td>BREAKING: FEDERAL COURT RULES ON NSA’S WARRANT...</td>\n      <td>Another positive step towards restoring our fr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>66148</th>\n      <td>China calls for restraint when asked about Nor...</td>\n      <td>BEIJING (Reuters) - China called on all partie...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6040 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(short_dataset_df)\nreal_train_ds = train_ds.filter(\n    lambda x: x[\"label\"] == 1\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:46:06.153548Z","iopub.execute_input":"2024-06-01T16:46:06.154258Z","iopub.status.idle":"2024-06-01T16:46:06.881643Z","shell.execute_reply.started":"2024-06-01T16:46:06.154221Z","shell.execute_reply":"2024-06-01T16:46:06.880632Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6040 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f99f74915d4e9f919073b602501409"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 1. Phi-2 + LoRA\nSource: https://medium.com/@prasadmahamulkar/fine-tuning-phi-2-a-step-by-step-guide-e672e7f1d009","metadata":{}},{"cell_type":"code","source":"# MODEL_NAME = \"microsoft/phi-2\"\nMODEL_NAME = \"./phi-2-finetuned-lora-new\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n    target_modules= [\"Wqkv\", \"out_proj\"]\n)\n\norig_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    revision=\"refs/pr/23\",\n)\norig_model.config.use_cache = False\norig_model.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\norig_model = prepare_model_for_kbit_training(orig_model, use_gradient_checkpointing=True)\nmodel = get_peft_model(orig_model, peft_config=lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:36:52.237574Z","iopub.execute_input":"2024-06-01T15:36:52.237959Z","iopub.status.idle":"2024-06-01T15:37:20.143739Z","shell.execute_reply.started":"2024-06-01T15:36:52.237931Z","shell.execute_reply":"2024-06-01T15:37:20.142726Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984a215629fb49508295a3df5d0f8ed2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e6e1758abe4fd38affcd09540b89cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/33.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42b9e8b1ca640c8bcf54ee9963774fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197ab52fb1924d05978d0dcae017af2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d94a54d095468290743cf600839ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1593484a244e39972c6657d3e98b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/577M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98fe491a9e2f4545802e2ec80bf4df7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ed4f54ec7f48a78569214161b1da6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc54eeb6ddf405cb34a0899ec2fa5eb"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:37:23.066435Z","iopub.execute_input":"2024-06-01T15:37:23.066808Z","iopub.status.idle":"2024-06-01T15:37:23.076681Z","shell.execute_reply.started":"2024-06-01T15:37:23.066781Z","shell.execute_reply":"2024-06-01T15:37:23.075634Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"trainable params: 31,457,280 || all params: 2,811,141,120 || trainable%: 1.1190\n","output_type":"stream"}]},{"cell_type":"code","source":"PREFIX = \"### Title:\"\nRESPONSE_TEMPLATE = \"\\n### Article:\"\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example[\"title\"])):\n        text = f\"{PREFIX} {example['title'][i]}. {RESPONSE_TEMPLATE} {example['text'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\n\ndef generate_phi2(model, tokenizer, title: str, max_new_tokens: int | None = None):\n    input_tokens = tokenizer(f\"{PREFIX} {title}. {RESPONSE_TEMPLATE}\", return_tensors=\"pt\")[\"input_ids\"]\n    input_tokens = input_tokens.to(device=model.device)\n    output_tokens = model.generate(\n        input_tokens, \n        max_new_tokens=max_new_tokens, \n        #eos_token_id=tokenizer.eos_token_id\n    )\n    return tokenizer.decode(output_tokens[0])\n\n\ncollator = DataCollatorForCompletionOnlyLM(RESPONSE_TEMPLATE, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:37:31.846116Z","iopub.execute_input":"2024-06-01T15:37:31.846757Z","iopub.status.idle":"2024-06-01T15:37:31.857341Z","shell.execute_reply.started":"2024-06-01T15:37:31.846729Z","shell.execute_reply":"2024-06-01T15:37:31.856446Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./training_results\", \n    report_to=\"none\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=2,\n    max_grad_norm=0.3,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    group_by_length=True,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=real_train_ds,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator,\n    args=training_args\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:37:37.137543Z","iopub.execute_input":"2024-06-01T15:37:37.138190Z","iopub.status.idle":"2024-06-01T15:37:49.717910Z","shell.execute_reply.started":"2024-06-01T15:37:37.138163Z","shell.execute_reply":"2024-06-01T15:37:49.716917Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf53e148e6f84b658d84c83d410f4fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8af6c8c72c546a1b56d1e740b12b659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e85165a9b01473996aa98c9bbca0057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca0cf90d10d463dad5f6ed9083c725a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf68f3f5fcfc4f4ab89b22fc67b06636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac18b064bf643888750959218dac259"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2804 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e57f791c194eb3a0e6d7a06c865176"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:37:52.242260Z","iopub.execute_input":"2024-06-01T15:37:52.242647Z","iopub.status.idle":"2024-06-01T16:06:17.964294Z","shell.execute_reply.started":"2024-06-01T15:37:52.242596Z","shell.execute_reply":"2024-06-01T16:06:17.962964Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='334' max='4206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 334/4206 28:08 < 5:28:15, 0.20 it/s, Epoch 0.24/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(generate_phi2(model, tokenizer, \"Belarus under new hard-hitting sanctions\", max_new_tokens=150))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:27:12.888427Z","iopub.execute_input":"2024-06-01T15:27:12.889021Z","iopub.status.idle":"2024-06-01T15:27:25.146543Z","shell.execute_reply.started":"2024-06-01T15:27:12.888987Z","shell.execute_reply":"2024-06-01T15:27:25.145599Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"### Title: Belarus under new hard-hitting sanctions. \n### Article: By David Icke The US and EU have imposed new sanctions on Belarus, in response to the country’s recent presidential election. The US has imposed sanctions on the country’s president Alexander Lukashenko, and the EU has imposed sanctions on the country’s prime minister, Svetlana Tsikhanouskaya. The sanctions are aimed at pressuring Lukashenko to step down, and to support Tsikhanouskaya, who is the first female president in the country’s history. The sanctions are a significant escalation in the US and EU’s efforts to pressure Lukashenko to step down. The sanctions are also a sign of the growing tension between the US and Russia, as well as between the\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.save_model(\"phi-2-finetuned-lora-new\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:19:31.676792Z","iopub.execute_input":"2024-06-01T15:19:31.677455Z","iopub.status.idle":"2024-06-01T15:19:32.227668Z","shell.execute_reply.started":"2024-06-01T15:19:31.677422Z","shell.execute_reply":"2024-06-01T15:19:32.226863Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"You are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Flan-T5","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"google/flan-t5-base\"\n\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:22:09.513036Z","iopub.execute_input":"2024-05-30T17:22:09.513889Z","iopub.status.idle":"2024-05-30T17:22:10.962718Z","shell.execute_reply.started":"2024-05-30T17:22:09.513854Z","shell.execute_reply":"2024-05-30T17:22:10.961910Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# We prefix our tasks with \"answer the question\"\nPREFIX = \"Please write an article based on the title: \"\n\n# Define the preprocessing function\n\ndef preprocess_function(examples):\n    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n    # The \"inputs\" are the tokenized answer:\n    inputs = [PREFIX + doc for doc in examples[\"title\"]]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n\n    # The \"labels\" are the tokenized outputs:\n    labels = tokenizer(text_target=examples[\"text\"], \n                      max_length=512,         \n                      truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Map the preprocessing function across our dataset\ntokenized_dataset = (\n    real_train_ds\n    .map(preprocess_function, batched=True)\n    .select_columns([\"input_ids\", \"labels\"])\n)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:30:21.721241Z","iopub.execute_input":"2024-05-30T17:30:21.722071Z","iopub.status.idle":"2024-05-30T17:30:23.680537Z","shell.execute_reply.started":"2024-05-30T17:30:21.722039Z","shell.execute_reply":"2024-05-30T17:30:23.679614Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2764 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b613d6a9c344b0a828e0fc938cdbfd"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 2764\n})"},"metadata":{}}]},{"cell_type":"code","source":"def generate_t5(model, tokenizer, title: str, max_new_tokens=100):\n    input_tokens = tokenizer(PREFIX + title, return_tensors=\"pt\")[\"input_ids\"]\n    input_tokens = input_tokens.to(device=model.device)\n    output_tokens = model.generate(input_tokens, max_new_tokens=max_new_tokens)\n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:33:53.246514Z","iopub.execute_input":"2024-05-30T17:33:53.246888Z","iopub.status.idle":"2024-05-30T17:33:53.254369Z","shell.execute_reply.started":"2024-05-30T17:33:53.246860Z","shell.execute_reply":"2024-05-30T17:33:53.253182Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    # eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    # compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T17:55:58.499368Z","iopub.execute_input":"2024-05-30T17:55:58.499768Z","iopub.status.idle":"2024-05-30T17:55:58.541785Z","shell.execute_reply.started":"2024-05-30T17:55:58.499739Z","shell.execute_reply":"2024-05-30T17:55:58.540861Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  deprecated_dl_args[\"split_batches\"] = split_batches\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:18:44.496043Z","iopub.execute_input":"2024-05-30T18:18:44.496640Z","iopub.status.idle":"2024-05-30T18:36:33.334423Z","shell.execute_reply.started":"2024-05-30T18:18:44.496607Z","shell.execute_reply":"2024-05-30T18:36:33.333538Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3460' max='3460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3460/3460 17:47, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.716900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.529400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.376400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.325000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3460, training_loss=0.48189539826674266, metrics={'train_runtime': 1068.0379, 'train_samples_per_second': 25.879, 'train_steps_per_second': 3.24, 'total_flos': 1953575278866432.0, 'train_loss': 0.48189539826674266, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"article_text = generate_t5(\n    model, \n    tokenizer, \n    \"China calls for restraint when asked about Norway\"\n)\narticle_text","metadata":{"execution":{"iopub.status.busy":"2024-06-01T15:17:35.551745Z","iopub.execute_input":"2024-06-01T15:17:35.552917Z","iopub.status.idle":"2024-06-01T15:17:36.089379Z","shell.execute_reply.started":"2024-06-01T15:17:35.552875Z","shell.execute_reply":"2024-06-01T15:17:36.088086Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m article_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_t5\u001b[49m(\n\u001b[1;32m      2\u001b[0m     model, \n\u001b[1;32m      3\u001b[0m     tokenizer, \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChina calls for restraint when asked about Norway\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m article_text\n","\u001b[0;31mNameError\u001b[0m: name 'generate_t5' is not defined"],"ename":"NameError","evalue":"name 'generate_t5' is not defined","output_type":"error"}]},{"cell_type":"code","source":"trainer.save_model(\"flan-t5-base-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:38:26.846613Z","iopub.execute_input":"2024-05-30T18:38:26.847295Z","iopub.status.idle":"2024-05-30T18:38:28.347645Z","shell.execute_reply.started":"2024-05-30T18:38:26.847261Z","shell.execute_reply":"2024-05-30T18:38:28.346639Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## 3. TinyLlama\n\nSource: https://www.kaggle.com/code/tommyadams/fine-tuning-tinyllama","metadata":{}},{"cell_type":"code","source":"# MODEL_acceleratorNAME = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\nMODEL_NAME = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,           \n    bnb_4bit_quant_type=\"nf4\",    \n    bnb_4bit_use_double_quant=True, \n    bnb_4bit_compute_dtype=torch.bfloat16, \n)\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Creating tokenizer and defining the pad token\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) \ntokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config, \n    device_map=\"auto\",  \n    trust_remote_code=True, \n)\nbase_model.config.use_cache = False\n\nbase_model = prepare_model_for_kbit_training(base_model)\npeft_model = get_peft_model(base_model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:00:51.731597Z","iopub.execute_input":"2024-06-01T17:00:51.731992Z","iopub.status.idle":"2024-06-01T17:00:55.917507Z","shell.execute_reply.started":"2024-06-01T17:00:51.731960Z","shell.execute_reply":"2024-06-01T17:00:55.916334Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"peft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:07:45.026121Z","iopub.execute_input":"2024-06-01T17:07:45.026517Z","iopub.status.idle":"2024-06-01T17:07:45.035986Z","shell.execute_reply.started":"2024-06-01T17:07:45.026482Z","shell.execute_reply":"2024-06-01T17:07:45.034825Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n","output_type":"stream"}]},{"cell_type":"code","source":"PREFIX = \"### Title:\"\nRESPONSE_TEMPLATE = \"\\n### Article:\"\n\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example[\"title\"])):\n        text = f\"{PREFIX} {example['title'][i]}. {RESPONSE_TEMPLATE} {example['text'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\n\ndef generate_tinyllama(model, tokenizer, title: str, max_new_tokens: int | None = None):\n    input_tokens = tokenizer(f\"{PREFIX} {title}. {RESPONSE_TEMPLATE}\", return_tensors=\"pt\")[\"input_ids\"]\n    input_tokens = input_tokens.to(device=model.device)\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens, \n        pad_token_id = tokenizer.eos_token_id,\n        repetition_penalty=1.3, \n        eos_token_id = tokenizer.eos_token_id\n    )\n    output_tokens = model.generate(\n        input_tokens, \n        generation_config=generation_config\n    )\n    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n\n\ncollator = DataCollatorForCompletionOnlyLM(RESPONSE_TEMPLATE, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:11:24.060330Z","iopub.execute_input":"2024-06-01T17:11:24.060755Z","iopub.status.idle":"2024-06-01T17:11:24.070758Z","shell.execute_reply.started":"2024-06-01T17:11:24.060727Z","shell.execute_reply":"2024-06-01T17:11:24.069751Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./tinyllama_results\",\n    per_device_train_batch_size=3,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=100,\n    num_train_epochs=3,\n    learning_rate=2e-3,\n    weight_decay=0.001,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    group_by_length=True,\n    report_to=\"none\"\n)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n#     peft_config=lora_config,\n    train_dataset=real_train_ds,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator,\n    args=training_args\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:23:52.915134Z","iopub.execute_input":"2024-06-01T17:23:52.916044Z","iopub.status.idle":"2024-06-01T17:23:54.039118Z","shell.execute_reply.started":"2024-06-01T17:23:52.916008Z","shell.execute_reply":"2024-06-01T17:23:54.038213Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2764 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1385904e09af4520a885db729c15cb81"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:23:54.040784Z","iopub.execute_input":"2024-06-01T17:23:54.041444Z","iopub.status.idle":"2024-06-01T17:48:40.730509Z","shell.execute_reply.started":"2024-06-01T17:23:54.041407Z","shell.execute_reply":"2024-06-01T17:48:40.729649Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1383' max='1383' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1383/1383 24:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.807200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.364300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.374700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.395800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.151400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.828900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.811900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.749000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.770400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.388200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.296900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.221500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.231900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1383, training_loss=1.7605743132375413, metrics={'train_runtime': 1485.4358, 'train_samples_per_second': 5.582, 'train_steps_per_second': 0.931, 'total_flos': 6091259555487744.0, 'train_loss': 1.7605743132375413, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"tinyllama-finetuned-lora\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:49:33.877860Z","iopub.execute_input":"2024-06-01T17:49:33.878570Z","iopub.status.idle":"2024-06-01T17:49:34.977594Z","shell.execute_reply.started":"2024-06-01T17:49:33.878530Z","shell.execute_reply":"2024-06-01T17:49:34.976586Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(generate_tinyllama(peft_model, tokenizer, \"Biden proposes studen debt relief bill\", max_new_tokens=150))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T17:51:16.535486Z","iopub.execute_input":"2024-06-01T17:51:16.536152Z","iopub.status.idle":"2024-06-01T17:51:30.970507Z","shell.execute_reply.started":"2024-06-01T17:51:16.536118Z","shell.execute_reply":"2024-06-01T17:51:30.969533Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"### Title: Biden proposes studen debt relief bill. \n### Article: Posted: Novena, pray for us, share with others  \n1-Novena to Prayer for US Senate : Biden proposes studen debt relief bill by Lizzie Mendoza over at The Blaze . We love this! Via: Daily Caller , Share with friends and family! \"Biden proposes Students’ Debt Relief Act.\" pic.twitter.com/6YFNdQX3ZV \n— Lizzie Mendoza (@lizzymendoza) October 27, 2016 \nThe DREAMS of a Biden presidency are endless student loan bubble that will swallow up every American who\n","output_type":"stream"}]}]}