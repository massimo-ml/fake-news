{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_news.classifier_evaluation import evaluate_classifiers\n",
    "from fake_news.classifiers import (\n",
    "    ConvolutionalNeuralNetworkClassifier,\n",
    "    LogisticRegressionNewsClassifier,\n",
    "    LSTMClassifier,\n",
    "    MultinomialNaiveBayesClassifier,\n",
    "    RandomForestClassifierClass,\n",
    "    RecurrentNeuralNetworkClassifier,\n",
    "    SupportVectorMachineClassifier\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIERS_DICT = {\n",
    "    \"logistic\": (LogisticRegressionNewsClassifier, \"ml\"),\n",
    "    \"naive_bayes\": (MultinomialNaiveBayesClassifier, \"ml\"),\n",
    "    \"random_forest\": (RandomForestClassifierClass, \"ml\"),\n",
    "    \"svm\": (SupportVectorMachineClassifier, \"ml\"),\n",
    "    \"cnn\": (ConvolutionalNeuralNetworkClassifier, \"dl\"),\n",
    "    \"rnn\": (RecurrentNeuralNetworkClassifier, \"dl\"),\n",
    "    \"lstm\": (LSTMClassifier, \"dl\"),\n",
    "}\n",
    "\n",
    "ORIG_CLASSIFIER_PATHS_DICT = {\n",
    "    \"logistic\": R\"../fake_news/classifiers/logisticregression.pkl\",\n",
    "    \"naive_bayes\": R\"../fake_news/classifiers/naivebayes.pkl\",\n",
    "    \"random_forest\": R\"../fake_news/classifiers/rf_model.pkl\",\n",
    "    \"svm\": R\"../fake_news/classifiers/svm_model.pkl\",\n",
    "    \"cnn\": R\"../fake_news/classifiers/cnn.keras\",\n",
    "    \"rnn\": R\"../fake_news/classifiers/rnn.keras\",\n",
    "    \"lstm\": R\"../fake_news/classifiers/lstm_model.keras\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"../data\")\n",
    "train_df = pd.read_csv(DATASET_DIR / \"WELFake_clean_train.csv\")\n",
    "test_df = pd.read_csv(DATASET_DIR / \"WELFake_clean_test.csv\")\n",
    "\n",
    "TOKENIZERS_DIR = Path(\"../fake_news/classifiers/tokenizers\")\n",
    "orig_tokenizer_paths = (\n",
    "    str(TOKENIZERS_DIR / \"ml_tokenizer.pickle\"),\n",
    "    str(TOKENIZERS_DIR / \"dl_tokenizer.pickle\") \n",
    ")\n",
    "synthetic_names = [\"tinyllama_real_articles.csv\"] ## CHANGE SYNTHETIC DATA HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Started working on tinyllama_real_articles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizers\n",
      "d:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "INFO:root:Started evaluating <class 'fake_news.classifiers.RNN.RecurrentNeuralNetworkClassifier'>\n",
      "INFO:root:Fitting and predicting on original data\n",
      "d:\\mamba\\envs\\fake-news\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "d:\\mamba\\envs\\fake-news\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ../fake_news/classifiers/rnn.keras\n",
      "\u001b[1m443/443\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting and predicting on combined data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 160ms/step - accuracy: 0.7455 - loss: 0.4942 - val_accuracy: 0.8700 - val_loss: 0.3040\n",
      "Epoch 2/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 162ms/step - accuracy: 0.9169 - loss: 0.2132 - val_accuracy: 0.9267 - val_loss: 0.2053\n",
      "Epoch 3/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 151ms/step - accuracy: 0.9537 - loss: 0.1232 - val_accuracy: 0.6892 - val_loss: 0.5394\n",
      "Epoch 4/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 157ms/step - accuracy: 0.8664 - loss: 0.2961 - val_accuracy: 0.9038 - val_loss: 0.2551\n",
      "Epoch 5/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 163ms/step - accuracy: 0.9680 - loss: 0.0924 - val_accuracy: 0.8968 - val_loss: 0.2825\n",
      "Epoch 6/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 149ms/step - accuracy: 0.9826 - loss: 0.0562 - val_accuracy: 0.9096 - val_loss: 0.2990\n",
      "Epoch 7/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 168ms/step - accuracy: 0.9856 - loss: 0.0450 - val_accuracy: 0.9121 - val_loss: 0.3110\n",
      "Epoch 8/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 152ms/step - accuracy: 0.9878 - loss: 0.0388 - val_accuracy: 0.9091 - val_loss: 0.3536\n",
      "Epoch 9/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 162ms/step - accuracy: 0.9928 - loss: 0.0239 - val_accuracy: 0.9097 - val_loss: 0.4260\n",
      "Epoch 10/10\n",
      "\u001b[1m733/733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 149ms/step - accuracy: 0.9906 - loss: 0.0281 - val_accuracy: 0.9007 - val_loss: 0.2801\n",
      "\u001b[1m443/443\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Calculating metrics\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [14159, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Started working on\u001b[39m\u001b[38;5;124m\"\u001b[39m, synthetic_name)\n\u001b[0;32m      9\u001b[0m synth_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATASET_DIR \u001b[38;5;241m/\u001b[39m synthetic_name)\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_classifiers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynth_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynth_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_tokenizer_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_tokenizer_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_tokenizer_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_tokenizer_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_classifier_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_classifier_paths\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m total_results[synthetic_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m     classif_name: {\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morig\u001b[39m\u001b[38;5;124m\"\u001b[39m: classif_result[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     30\u001b[0m }\n",
      "File \u001b[1;32md:\\Documents\\University\\Semester6\\NLP\\fake-news\\notebooks\\..\\fake_news\\classifier_evaluation.py:114\u001b[0m, in \u001b[0;36mevaluate_classifiers\u001b[1;34m(classifiers, train_df, synth_df, test_df, metrics, orig_tokenizer_paths, combined_tokenizer_paths, nltk_data_path, orig_classifier_paths)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    112\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m orig_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m combined_metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(\n\u001b[0;32m    118\u001b[0m     test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, combined_pred, metrics\u001b[38;5;241m=\u001b[39mmetrics\n\u001b[0;32m    119\u001b[0m )\n\u001b[0;32m    120\u001b[0m metrics_results\u001b[38;5;241m.\u001b[39mappend((orig_metrics, combined_metrics))\n",
      "File \u001b[1;32md:\\Documents\\University\\Semester6\\NLP\\fake-news\\notebooks\\..\\fake_news\\metrics.py:19\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(y_true, y_pred, metrics)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m metric:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m         res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     21\u001b[0m         res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m roc_auc_score(y_true, y_pred)\n",
      "File \u001b[1;32md:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\metrics\\_classification.py:222\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\metrics\\_classification.py:99\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\mamba\\envs\\fake-news\\lib\\site-packages\\sklearn\\utils\\validation.py:460\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    458\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    463\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [14159, 1]"
     ]
    }
   ],
   "source": [
    "classifiers_to_evaluate = [\"rnn\"] ## CHANGE MODELS HERE\n",
    "classifiers = [CLASSIFIERS_DICT[name] for name in classifiers_to_evaluate]\n",
    "orig_classifier_paths = [ORIG_CLASSIFIER_PATHS_DICT[name] for name in classifiers_to_evaluate]\n",
    "\n",
    "total_results = {}\n",
    "\n",
    "for synthetic_name in synthetic_names:\n",
    "    print(\"=== Started working on\", synthetic_name)\n",
    "    synth_df = pd.read_csv(DATASET_DIR / synthetic_name)\n",
    "\n",
    "    results = evaluate_classifiers(\n",
    "        classifiers=classifiers, \n",
    "        train_df=train_df,\n",
    "        synth_df=synth_df,\n",
    "        test_df=test_df,\n",
    "        metrics=[\"acc\", \"auc\", \"f1\"],\n",
    "        orig_tokenizer_paths=orig_tokenizer_paths,\n",
    "        combined_tokenizer_paths=orig_tokenizer_paths,\n",
    "        orig_classifier_paths=orig_classifier_paths\n",
    "    )\n",
    "\n",
    "    total_results[synthetic_name] = {\n",
    "        classif_name: {\n",
    "            \"orig\": classif_result[0],\n",
    "            \"combined\": classif_result[1]\n",
    "        } \n",
    "        for classif_name, classif_result in zip(\n",
    "            classifiers_to_evaluate, results\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"total_results.json\", \"w\") as f:\n",
    "    json.dump(total_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
